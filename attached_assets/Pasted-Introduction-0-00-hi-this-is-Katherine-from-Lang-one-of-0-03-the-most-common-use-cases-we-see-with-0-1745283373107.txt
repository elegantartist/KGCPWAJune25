Introduction
0:00
hi this is Katherine from Lang one of
0:03
the most common use cases we see with
0:05
LMS is code generation but here's the
0:08
question how do you improve the accuracy
0:10
and performance of the code generated by
0:12
your code agent one answer is static
0:15
analysis checking the code for errors or
0:18
for inconsistencies without executing it
0:21
this is where tools like Pyate or My Pi
0:23
comes in so now you could actually build
0:25
that kind of check into your agent loop
0:28
this is what we call a reflection step a
0:31
point in the loop where the agent looks
0:33
back at what it just generated and
0:35
validates or improves it before
0:37
continuing we'll show you an example of
0:39
how you can create such a reflection
0:41
step in your architecture evaluating its
0:43
own output before returning a result
0:46
instead of building evaluation logic
0:47
from scratch we'll show you how you can
0:50
leverage open evals an open- source
0:52
package that provides you with out-
0:53
of-the-box tools for things like type
0:56
checking and sandbox
0:58
evaluation open evals includes a number
OpenEvals code evaluator introduction
1:00
of helpful out ofthe-box evaluators this
1:03
includes ones form as a judge rack and
1:08
code
1:08
evaluation today we're going to focus
1:11
specifically on the code evaluator
1:13
section open evals gives you several
1:16
pre-built evaluators for generated code
1:18
such as type-checking generated code
1:21
sandbox type checking and execution
1:24
evaluators and finally using Elm as a
1:27
judge to evaluate code
1:29
quality since LM outputs often contain
1:32
both code and non-code text open
1:35
evaluators also include flexible code
1:38
extraction utilities you can either use
1:41
an LLM to extract relevant snippets or
1:44
simply extract markdown code blocks
1:46
directly now let's deep dive into the
1:48
several techniques that we can take to
1:50
validate code first you can type check
1:53
generated code with Pyite and my PI
1:55
these are lightweight approaches to type
1:57
check Python code but they won't install
2:00
or check for missing dependencies for
2:02
TypeScript there's a similar evaluator
2:04
for type checking second you can use LM
2:07
as a judge to prompt LM to provide
2:09
feedback on your code lm as a judge
2:12
evaluator here defaults to no code
2:14
extraction strategy but it gives the
2:16
option to enable extracting code which
2:19
could be helpful to reduce noise or
2:21
distraction especially when using
2:22
smaller models finally open evals
2:26
integrates with E2B to run some code
2:28
evaluators in isolated sandboxes
2:31
static analysis is a useful tool for
2:34
caching mistakes in generated code open
2:36
evals lets you perform this fully
2:38
locally sometimes generated code can
2:41
contain and misuse third party libraries
2:43
so rather than installing or executing
2:46
these arbitrary dependencies locally you
2:48
can use the sandbox environment to
2:50
effectively perform type checking there
2:52
are two key sandbox evaluators first is
2:54
the sandbox typeing evaluators one
2:57
example is the sandbox pyate this
2:59
evaluator parses out the required
3:01
dependencies installs them in a sandbox
3:04
and runs pyate it returns any
3:06
type-checking errors found for
3:08
TypeScript similarly there's a
3:11
type-checking sandbox
3:12
evaluator the second type of sandbox
3:15
evaluator is the sandbox execution
3:18
evaluator this evaluator goes a step
3:20
beyond installing dependencies actually
3:23
running the code inside the sandbox and
3:25
checking for runtime errors now let's
3:27
take a look at the example we saw
3:29
earlier to see how it's applied in
3:31
action now back to the example we saw in
Example implementation of reflection step in the loop - Mini Chat LangChain
3:33
the beginning the base agent has a
3:36
simple React architecture implementation
3:39
where it works off of a text file
3:40
embedded in the system prompt and it has
3:43
tools to fetch further
3:45
information to improve the correctness
3:47
of generated code mini chat link chain
3:49
includes a reflection set which verifies
3:52
the generated code through the sandbox
3:54
typeing evaluator in the loop let's take
3:56
a look at how it's
3:59
constructed so this is our reflection
4:01
node in our reflection step we first
4:04
create a sandbox pyite co-evaluator
4:07
where the evaluator extracts generated
4:09
code from the agent's output through
4:10
markdown and pushes it up to an E2B
4:13
sandbox to run Pyrite over it we
4:16
obtained a result from running the
4:17
evaluator and now there are two
4:19
situations where we will want to
4:21
directly output the response to the end
4:23
user the first scenario is if the code
4:26
generated does not have any
4:28
type-checking issues that is within the
4:31
result score field it outputs true so it
4:33
wouldn't need any code regeneration the
4:36
second scenario is if the original agent
4:38
response did not include any code at all
4:41
we can check for this by checking the
4:43
result metadata to see if code
4:44
extraction step in the evaluator field
4:47
in this case we also want to directly
4:49
output our agents response
4:52
so now transitioning that logic into
4:54
code in the case that our agent's
4:56
response contains code and finds false
4:59
evaluator results we want to feed the
5:02
error message back to the agent to
5:04
regenerate code we do so by attaching a
5:07
user message to the LM giving context on
5:10
the errors found and directions to
5:12
regenerate code with this information in
5:14
the prompt the agent then has to inform
5:16
context to regenerate and modify based
5:19
on the error
5:20
let's try this
5:22
out i can feed an example input to our
5:25
agent asking to create a swarm style
5:29
agent the query is routed to our agent
5:32
node which generates its respond while
5:35
calling a set of tools once the response
5:38
is finished generating this is passed to
5:40
a reflection node to create the results
5:44
looks like the reflection node has
5:45
caught some errors which is now
5:47
prompting the agent to regenerate the
5:50
code now our agent regenerates the
5:52
response which now passes the evaluator
5:55
test and their outputs our final answer
5:59
we can also inspect the trace and lang
6:01
to take a detailed look at the run
6:04
behavior so first we can see that the
6:06
base agent called a get langraph docs
6:09
content tool this fetches relevant
6:12
content from relevant
6:14
documents and then it calls the LLM to
6:18
generate a response the response is then
6:21
passed to the reflection step where the
6:23
evaluator is run taking a closer look at
6:26
the evaluator results it passed through
6:29
a score of false and based on the error
6:33
message several required arguments seems
6:35
to be missing so this includes model
6:37
name timeout and stop so this is then
6:40
passed on as a part of the human message
6:44
if we scroll down where we included the
6:46
error found and asking the agent to try
6:48
to fix it so now having the context of
6:51
the error message the agent generates
6:54
another round of outputs the generated
6:56
output is then passed to the reflection
6:59
step and during the second time of
7:00
running the evaluator we can see that no
7:03
errors were found returning true and
7:05
from there the final response is passed
7:07
on to the customer
7:09
adding this reflection step within our
7:11
architecture this helps catch some of
7:13
the type checking mistakes that are
7:15
being made by the LM and this can ensure
7:18
better code quality for our agent thank
7:20
you for watching